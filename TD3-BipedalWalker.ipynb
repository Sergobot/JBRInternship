{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from tqdm.notebook import tqdm\n",
    "from my_utils import ReplayBuffer, construct_nn, Logger\n",
    "from time import time\n",
    "import os\n",
    "import pathlib\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    '''\n",
    "    Actor performs acitons in the enviroment, taking the observation as an input.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        obs_dim (int): Shape of the observation\n",
    "        act_dim (int): Action space shape\n",
    "        sizes (list of ints): Sizes of hidden layers, in order from observation to action\n",
    "    '''\n",
    "    def __init__(self, obs_dim, act_dim, sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pi = construct_nn([obs_dim] + sizes + [act_dim], nn.Tanh)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        '''\n",
    "        Process observation and choose an action\n",
    "        \n",
    "        Args:\n",
    "            obs (torch.tensor): the environment state to take an action in.\n",
    "        '''\n",
    "        return self.pi(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    '''\n",
    "    Critic judges how good an action is, given preceding observation and action itself.\n",
    "    Its goal is to compute the precise Q-value of a state.\n",
    "    \n",
    "    Args:\n",
    "        obs_dim (int): Shape of the observation\n",
    "        act_dim (int): Action space shape\n",
    "        sizes (list of ints): Sizes of hidden layers, in order from input to output\n",
    "\n",
    "    '''\n",
    "    def __init__(self, obs_dim, act_dim, sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.q = construct_nn([obs_dim + act_dim] + sizes + [1])\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        return self.q(torch.cat([obs, act], -1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(nn.Module):\n",
    "    '''\n",
    "    TD3 as a class incorporates the three networks described in the paper.\n",
    "    There's a single Actor and two Critics, whose joint goal is to judge Actor's actions more precisely.\n",
    "    \n",
    "    Args:\n",
    "        obs_dim (int): Shape of the observation\n",
    "        act_dim (int): Action space shape\n",
    "        sizes (list of ints): Sizes of hidden layers, in order from input to output. Same for the Actor and Critics.\n",
    "    '''\n",
    "    def __init__(self, obs_dim, act_dim, sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = Actor(obs_dim, act_dim, sizes)\n",
    "        self.critic1 = Critic(obs_dim, act_dim, sizes)\n",
    "        self.critic2 = Critic(obs_dim, act_dim, sizes)\n",
    "    \n",
    "    def act(self, obs):\n",
    "        '''\n",
    "        Choose best action according to the Actor's policy\n",
    "        \n",
    "        Args:\n",
    "            obs (torch.tensor): the state to perform an action in\n",
    "            \n",
    "        Returns:\n",
    "            (torch.tensor): Actor's action\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            return self.actor(obs)\n",
    "    \n",
    "    def criticize(self, obs, act):\n",
    "        '''\n",
    "        Let critics judge how good an action is given the observation.\n",
    "        \n",
    "        Args:\n",
    "            obs (torch.tensor): the state that the action was performed in\n",
    "            act (torch.tensor): the action\n",
    "        '''\n",
    "        return self.critic1(obs, act), self.critic2(obs, act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set some hyperparameters!\n",
    "These are mostly taken from the paper, and are either the values recommended by authors, or well-suitable ones for the problem (like network size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "POLYAK = 0.995\n",
    "POLICY_UPDATE_EVERY = 2\n",
    "NETWORK_SIZES = [256, 256]\n",
    "MAX_LEN = 1600\n",
    "EPOCHS = 13 # About a million steps\n",
    "EPISODES_PER_EPOCH = 50\n",
    "RENDER_PER_EPOCH = 1 # Number of episodes to render every epoch. Footage is saved to the disk.\n",
    "NOISE_LIMIT = 0.5\n",
    "NOISE_STD = 0.2\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "def avg(l): return sum(l) / len(l)\n",
    "logger = Logger()\n",
    "logger.add_attribute('ret', [max, avg])\n",
    "logger.add_attribute('len', [max, avg])\n",
    "logger.add_attribute('critics_loss', [avg])\n",
    "logger.add_attribute('actor_loss', [avg])\n",
    "logger.add_attribute('q1', [avg])\n",
    "logger.add_attribute('q2', [avg])\n",
    "\n",
    "# Upon each run, a new directory is created. There will be stored model snaphosts,\n",
    "# taken at the end of every epoch, tensorboard logs and video footage of training process.\n",
    "RUN_ID = str(time())\n",
    "RUN_PATH = f'./td3/{RUN_ID}'\n",
    "if not os.path.exists(RUN_PATH) or not os.path.isdir(RUN_PATH):\n",
    "    pathlib.Path(RUN_PATH).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "TENSORBOARD_PATH = f'./td3/tensorboard'\n",
    "writer = SummaryWriter(f'{TENSORBOARD_PATH}/{RUN_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergobot/miniconda3/envs/jbr/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v3')\n",
    "env.seed(SEED)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "act_high = torch.as_tensor(env.action_space.high, dtype=torch.float32).to(dev)\n",
    "act_low = torch.as_tensor(env.action_space.low, dtype=torch.float32).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This global variable is updated inside the training loop and indicates whether current episode\n",
    "# should be rendered and saved to disk or not.\n",
    "RENDER_THIS = False\n",
    "env = wrappers.Monitor(env, f'{RUN_PATH}/videos/', video_callable=lambda episode_id: RENDER_THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(obs_dim, act_dim, size=5 * EPISODES_PER_EPOCH * MAX_LEN, dev=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TD3(\n",
       "  (actor): Actor(\n",
       "    (pi): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=4, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (critic1): Critic(\n",
       "    (q): Sequential(\n",
       "      (0): Linear(in_features=28, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (5): Identity()\n",
       "    )\n",
       "  )\n",
       "  (critic2): Critic(\n",
       "    (q): Sequential(\n",
       "      (0): Linear(in_features=28, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (5): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the online network and the offline (target) one.\n",
    "# They are initialized to the same random weights.\n",
    "td3 = TD3(obs_dim, act_dim, NETWORK_SIZES).to(dev)\n",
    "td3_target = TD3(obs_dim, act_dim, NETWORK_SIZES).to(dev)\n",
    "td3_target.load_state_dict(td3.state_dict())\n",
    "td3_target.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sub-network of TD3 has its own optimizer, though the critics' ones will only be used together.\n",
    "actor_optimizer = optim.Adam(td3.actor.parameters())\n",
    "critic1_optimizer = optim.Adam(td3.critic1.parameters())\n",
    "critic2_optimizer = optim.Adam(td3.critic2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing fancy here :)\n",
    "def compute_loss(q1, q2, q_exp):\n",
    "    return F.mse_loss(q1, q_exp) + F.mse_loss(q2, q_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The action is selected by the actor of provided TD3 and then we add some noise to it,\n",
    "# sampled from normal distribution with center 0 and std of 0.2\n",
    "def select_action(obs, network, noisy=True):\n",
    "    act = network.act(obs)\n",
    "    if noisy:\n",
    "        noise = torch.randn_like(act) * NOISE_STD\n",
    "        noise.clamp_(-NOISE_LIMIT, NOISE_LIMIT)\n",
    "        return torch.min(torch.max(act + noise, act_low), act_high)\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not update the actor every single time: authors recommend doing it half as often\n",
    "def optimize(update_actor=False):\n",
    "    if len(buffer) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    batch = buffer.sample_batch(BATCH_SIZE)\n",
    "    obs, act, rew, next_obs, done = \\\n",
    "            batch['obs'], batch['act'], batch['rew'], batch['next_obs'], batch['done']\n",
    "    \n",
    "    next_act = select_action(next_obs, td3_target)\n",
    "    with torch.no_grad():\n",
    "        # Choose the minimum of two Q-values\n",
    "        q_exp = torch.min(*td3_target.criticize(next_obs, next_act)) * GAMMA * (1 - done) + rew\n",
    "\n",
    "    # Compute loss of online network's Q-values to the smaller of target network's ones\n",
    "    q1, q2 = td3.criticize(obs, act)\n",
    "    critics_loss = compute_loss(q1, q2, q_exp)\n",
    "    \n",
    "    logger.put('critics_loss', critics_loss.item())\n",
    "    logger.put('q1', q1.mean().item())\n",
    "    logger.put('q2', q2.mean().item())\n",
    "    \n",
    "    # As stated previously, despite having separate optimizers for the critics,\n",
    "    # they are executed together.\n",
    "    critic1_optimizer.zero_grad()\n",
    "    critic2_optimizer.zero_grad()\n",
    "    critics_loss.backward()\n",
    "    critic1_optimizer.step()\n",
    "    critic2_optimizer.step()\n",
    "    \n",
    "    if update_actor:\n",
    "        actor_loss = -td3.critic1(obs, td3.actor(obs)).mean()\n",
    "        logger.put('actor_loss', actor_loss.item())\n",
    "        \n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        \n",
    "        # Update target model with regards to the online one.\n",
    "        with torch.no_grad():\n",
    "            for p, p_target in zip(td3.actor.parameters(), td3_target.actor.parameters()):\n",
    "                p_target.data.mul_(POLYAK)\n",
    "                p_target.data.add_((1 - POLYAK) * p.data)\n",
    "            for p, p_target in zip(td3.critic1.parameters(), td3_target.critic1.parameters()):\n",
    "                p_target.data.mul_(POLYAK)\n",
    "                p_target.data.add_((1 - POLYAK) * p.data)\n",
    "            for p, p_target in zip(td3.critic2.parameters(), td3_target.critic2.parameters()):\n",
    "                p_target.data.mul_(POLYAK)\n",
    "                p_target.data.add_((1 - POLYAK) * p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7340), started 0:00:55 ago. (Use '!kill 7340' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5aa286b5a62c9492\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5aa286b5a62c9492\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This notebook embeds a tensorboard!\n",
    "%tensorboard --logdir $TENSORBOARD_PATH --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724e129d20f14702ab85d40d7e498988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[0]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0] ret_max=-104.4224; ret_avg=-122.3216; len_max=1600.0000; len_avg=171.2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36220b17e26943a9bb82827617d971ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[1]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] ret_max=-83.6791; ret_avg=-115.2220; len_max=1102.0000; len_avg=174.3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e713d95b6f44171ae347516ec7f2348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[2]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] ret_max=-43.3383; ret_avg=-116.3471; len_max=1600.0000; len_avg=693.5600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09dc8efc0b148fd8efe328a0563e83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[3]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3] ret_max=244.2094; ret_avg=-13.2133; len_max=1600.0000; len_avg=888.5200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6ec4d245cd401fbcaae425a2ae9a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[4]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4] ret_max=277.3995; ret_avg=138.6303; len_max=1554.0000; len_avg=910.4400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdb49e3dacc445e8aa39837c4de9e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[5]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5] ret_max=290.4075; ret_avg=122.8555; len_max=1133.0000; len_avg=691.5200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a48428f34334ae892c91e50417e70e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[6]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6] ret_max=292.8721; ret_avg=148.2164; len_max=996.0000; len_avg=686.3800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34353273464b481eae183ab5a8ce80e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[7]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7] ret_max=293.1718; ret_avg=149.7821; len_max=968.0000; len_avg=676.2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0872e02380fd44a08ee9dbe03717c81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[8]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8] ret_max=296.9715; ret_avg=245.9381; len_max=931.0000; len_avg=806.0400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cabc40dc87f443d8eccd69176559e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[9]', max=50.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9] ret_max=301.2549; ret_avg=197.1258; len_max=890.0000; len_avg=696.8800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff45ebd1b10a4b5aafa560e87bc16555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[10]', max=50.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[10] ret_max=302.2743; ret_avg=207.4384; len_max=877.0000; len_avg=695.7000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df86c5aa62b94b2fa2f185a500fa6f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[11]', max=50.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11] ret_max=305.1095; ret_avg=226.9158; len_max=856.0000; len_avg=713.6400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ff831ea92f4a24b9768cd971dcaf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[12]', max=50.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[12] ret_max=304.6622; ret_avg=280.2831; len_max=896.0000; len_avg=793.3600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f086cb28b8d040a7b2122b244ce2f528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='[13]', max=50.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[13] ret_max=302.7494; ret_avg=289.1408; len_max=1106.0000; len_avg=852.1400\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    for episode in tqdm(range(EPISODES_PER_EPOCH), desc=f'[{epoch}]'):\n",
    "        # Do not render all the episode, as it would take too much time:\n",
    "        # instead, only render $RENDER_PER_EPOCH ones every epoch\n",
    "#         RENDER_THIS = True if episode % (EPISODES_PER_EPOCH // RENDER_PER_EPOCH) == 0 else False\n",
    "        \n",
    "        obs = torch.as_tensor(env.reset(), dtype=torch.float32).to(dev)\n",
    "        ep_ret = 0\n",
    "        ep_len = 0        \n",
    "        for t in count():\n",
    "            act = select_action(obs, td3)\n",
    "            \n",
    "            next_obs, rew, done, _ = env.step(act.cpu().numpy())\n",
    "            next_obs = torch.as_tensor(next_obs, dtype=torch.float32).to(dev)\n",
    "            done = False if ep_len == MAX_LEN else done\n",
    "\n",
    "            ep_ret += rew\n",
    "            ep_len += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            if RENDER_THIS:\n",
    "                env.render()\n",
    "            \n",
    "            buffer.put(obs, act, rew, next_obs, done)\n",
    "            obs = next_obs\n",
    "            \n",
    "            optimize(total_steps % POLICY_UPDATE_EVERY)\n",
    "            \n",
    "            if done or ep_len == MAX_LEN:\n",
    "                break\n",
    "        logger.put('ret', ep_ret)\n",
    "        logger.put('len', ep_len)\n",
    "\n",
    "        tb_data = logger.summarize(attributes=['critics_loss', 'actor_loss', 'q1', 'q2'], fmt=False)\n",
    "        ep_id = epoch * EPISODES_PER_EPOCH + episode\n",
    "        for (attr, val) in tb_data:\n",
    "            writer.add_scalar(attr, val, ep_id)\n",
    "        writer.add_scalar('return', ep_ret, ep_id)\n",
    "        writer.add_scalar('length', ep_len, ep_id)\n",
    "        writer.flush()\n",
    "\n",
    "    print(f'[{epoch}] {logger.summarize(attributes=[\"ret\", \"len\"])}')\n",
    "    torch.save(td3.state_dict(), f'{RUN_PATH}/td3-epoch-{epoch}.pt')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
